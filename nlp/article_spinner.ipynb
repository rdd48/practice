{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open('input/bbc_text_cls.csv') as f:\n",
    "    running_art = ''\n",
    "    for l in f.readlines()[1:]:\n",
    "        if '\",' not in l or '\", ' in l:\n",
    "            running_art += f'{l.strip()} '\n",
    "        else:\n",
    "            last_line = l.split('\",')\n",
    "            assert len(last_line) == 2\n",
    "            articles.append(running_art + last_line[0].strip())\n",
    "            labels.append(last_line[1].strip())\n",
    "            running_art = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:'\"\\,<>/?@#^&*_~'''\n",
    "\n",
    "def remove_punc(s):\n",
    "    ''' takes in a string, removes all \"useless\" punctuation, lowercases it '''\n",
    "    no_punc = ''\n",
    "    for char in s:\n",
    "        if char not in punc:\n",
    "            no_punc += char.lower()\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_to_mm(articles):\n",
    "    # mm structure will be mm[(word_idx-1, word_idx+1)] = [list of words that fit this pattern]\n",
    "    # for start of line, will use an initial state distribution with structure isd\n",
    "    isd = {}\n",
    "    mm = {}\n",
    "\n",
    "    for article in articles:\n",
    "        words = remove_punc(article).split()\n",
    "        assert len(words) > 0\n",
    "\n",
    "        \n",
    "        for idx, w in enumerate(words):\n",
    "\n",
    "            # first word in line for generating the isd\n",
    "            if idx == 0:\n",
    "                next_w = words[idx+1]\n",
    "                if next_w not in isd:\n",
    "                    isd[next_w] = []\n",
    "                isd[next_w].append(w)\n",
    "            \n",
    "            # generate mm for all words before the end\n",
    "            # but add an END token if it's the end of a sentence\n",
    "            elif idx < len(words) - 1:\n",
    "                prev_w = words[idx-1] # if '.' not in w else 'START'\n",
    "                next_w = words[idx+1] # if '.' not in w else 'END'\n",
    "\n",
    "                if (prev_w, next_w) not in mm:\n",
    "                    mm[(prev_w, next_w)] = []\n",
    "                mm[(prev_w, next_w)].append(w)\n",
    "            \n",
    "            elif idx == len(words) -1:\n",
    "                prev_w = words[idx-1]\n",
    "                next_w = 'END'\n",
    "                if (prev_w, next_w) not in mm:\n",
    "                    mm[(prev_w, next_w)] = []\n",
    "                mm[(prev_w, next_w)].append(w)\n",
    "    \n",
    "    return isd, mm\n",
    "\n",
    "isd, mm = articles_to_mm(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dict(d):\n",
    "    ''' convert list of words in isd to dict of word probs '''\n",
    "    d_norm = {}\n",
    "    for k,v in d.items():\n",
    "        # case where there's only one word to choose from\n",
    "        if len(v) == 1:\n",
    "            d_norm[k] = {v[0]: 1.}\n",
    "        \n",
    "        # multiple words\n",
    "        elif len(v) > 1:\n",
    "            count_dict = {}\n",
    "            for w in v:\n",
    "                count_dict[w] = v.count(w)\n",
    "            \n",
    "            prob_dict = {}\n",
    "            cum_sum = sum(count_dict.values())\n",
    "            for count_k,count_v in count_dict.items():\n",
    "                prob_dict[count_k] = count_v / cum_sum\n",
    "            \n",
    "            d_norm[k] = prob_dict\n",
    "        \n",
    "    return d_norm\n",
    "\n",
    "isd_norm = normalize_dict(isd)\n",
    "mm_norm = normalize_dict(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_from_dist(d):\n",
    "    # d is a dict with form {key: prob, k2: p2 ... }\n",
    "    return np.random.choice(list(d.keys()), p=list(d.values()))\n",
    "\n",
    "def get_random_article(articles):\n",
    "    return remove_punc(np.random.choice(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euus seeking spyware trojan microsoft is not a trojan program that listen to write to the firms antispyware software. The new tool was only way by claims made the past few players cook has been masterminded by six million people. Stephen baker a fund manager at risk because the antispyware program was called bankasha trojan and is being heralded as an email attachment. Microsoft said politicians does not contain the poll was suspended and recommended users to 30% an antivirus program. The program attempts to viruses or delete microsofts antispyware tool and a warning messages or to use it will also likely to play online banking passwords or other personal information by net users keystrokes. Microsoft said that a lot today is preparing what he is a criminal offences on its software. Earlier this year he said it would buy security software maker sybari software to make its doors in which engines and email attachment. Microsoft has said the comes to revise its own paidfor antivirus software but none is not yet set a survey for its endangering the malicious program being targeted is not starring in the form and according to analysts. People find and adaware. Spyware programs constantly monitor internet use causes advert popups and imposed a bad performance.\n"
     ]
    }
   ],
   "source": [
    "my_article = get_random_article(articles).split()\n",
    "new_article = []\n",
    "for idx, word in enumerate(my_article):\n",
    "    if idx == 0:\n",
    "        next_word = my_article[idx+1]\n",
    "        new_word = pick_from_dist(isd_norm[next_word])\n",
    "    \n",
    "    elif idx < len(my_article) - 1:\n",
    "        prev_word = my_article[idx-1]\n",
    "        next_word = my_article[idx+1]\n",
    "        new_word = pick_from_dist(mm_norm[(prev_word, next_word)])\n",
    "    \n",
    "    elif idx == len(my_article) - 1:\n",
    "        prev_word = my_article[idx-1]\n",
    "        next_word = 'END'\n",
    "        new_word = f'{pick_from_dist(mm_norm[(prev_word, next_word)])}. '\n",
    "    \n",
    "    new_article.append(new_word)\n",
    "\n",
    "# the -2 removes the trailing period\n",
    "new_article_str = ' '.join(new_article)[:-2]\n",
    "\n",
    "# capitalize first letter\n",
    "test = [x.capitalize() for x in new_article_str.split('. ')]\n",
    "final_art = '. '.join(test)\n",
    "print(final_art)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb92fd372a38b58e9d9a6055cb8e345cfb54abe6393cd0e132548b13249ccdef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
